{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "your X_train and X_test should be of the shape (n_samples, 1, seq_len=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = [np.load(f'data/HandMovementDirection/{variable}_{set_name}.npy')\n",
    "        for variable in ['X', 'y'] for set_name in ['train', 'test']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = data\n",
    "\n",
    "print(\"X_train dims: \", X_train.shape)\n",
    "print(\"X_test dims: \", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if original sequence length is different, resize it, for example, using the following function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def resize(X):\n",
    "    X_scaled = F.interpolate(torch.tensor(X, dtype=torch.float), size=512, mode='linear', align_corners=False)\n",
    "    return X_scaled.numpy()\n",
    "    \n",
    "X_train, X_test = resize(X_train), resize(X_test)\n",
    "\n",
    "print(\"X_train dims: \", X_train.shape)\n",
    "print(\"X_test dims: \", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mantis.architecture import Mantis8M\n",
    "    \n",
    "device = 'cpu' # set device\n",
    "network = Mantis8M(device=device) # init model\n",
    "network = network.from_pretrained(\"paris-noah/Mantis-8M\") # load weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning without an adapter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, `adapter=None` for the `.fit()` method, which means that each channel is independently sent to the foundation model during the forward pass.\n",
    "All the outputs are further concatenated in a flattened manner and sent to the classification head."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "initialize the trainer and some arguments to pass during fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mantis.trainer import MantisTrainer\n",
    "\n",
    "model = MantisTrainer(device=device, network=network)\n",
    "\n",
    "# initialize some training parameters\n",
    "def init_optimizer(params): return torch.optim.AdamW(\n",
    "    params, lr=2e-4, betas=(0.9, 0.999), weight_decay=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning a classification head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuning_type = 'head'\n",
    "\n",
    "# fine-tune the model\n",
    "model.fit(X_train, y_train, num_epochs=100,\n",
    "            fine_tuning_type=fine_tuning_type, init_optimizer=init_optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evaluate performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "print(f'Accuracy on the test set is {np.mean(y_test == y_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuning_type = 'full'\n",
    "\n",
    "# fine-tune the model\n",
    "model.fit(X_train, y_train, num_epochs=100,\n",
    "            fine_tuning_type=fine_tuning_type, init_optimizer=init_optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evaluate performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "print(f'Accuracy on the test set is {np.mean(y_test == y_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract deep features with an adapter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the number of channels is too large and there is no access to multiple or powerful GPUs, the backpropagation step may be too costly.\n",
    "In this case, one can try to reduce dimension first and then send the transformed input to the foundation model.\n",
    "\n",
    "Here, we will use PCA, please refer to `multichannel_adapters.ipynb` for more examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply adapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mantis.adapters import MultichannelProjector\n",
    "\n",
    "adapter = MultichannelProjector(new_num_channels=5, patch_window_size=1, base_projector='pca')\n",
    "adapter.fit(X_train)\n",
    "X_reduced_train, X_reduced_test = adapter.transform(X_train), adapter.transform(X_test)\n",
    "\n",
    "print(\"X_reduced_train dims: \", X_reduced_train.shape)\n",
    "print(\"X_reduced_test dims: \", X_reduced_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning a classification head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuning_type = 'head'\n",
    "\n",
    "# fine-tune the model\n",
    "model.fit(X_reduced_train, y_train, num_epochs=100,\n",
    "            fine_tuning_type=fine_tuning_type, init_optimizer=init_optimizer)\n",
    "\n",
    "# evaluate performance\n",
    "y_pred = model.predict(X_reduced_test)\n",
    "print(f'Accuracy on the test set is {np.mean(y_test == y_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuning_type = 'full'\n",
    "\n",
    "# fine-tune the model\n",
    "model.fit(X_reduced_train, y_train, num_epochs=100,\n",
    "            fine_tuning_type=fine_tuning_type, init_optimizer=init_optimizer)\n",
    "\n",
    "# evaluate performance\n",
    "y_pred = model.predict(X_reduced_test)\n",
    "print(f'Accuracy on the test set is {np.mean(y_test == y_pred)}')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
